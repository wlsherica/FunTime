# 2015-12-29 Google tensorflow and deep learning 

# Google brain project
- Large dataset
- Large amounts of computation

# What is ML?
- AI and ML, a smart program for AI
- e.g. spam detection? the old way is writing a program with explicit rules to follow, however, the new way will be a program to learn from examples. try to classify email, etc.

# What is DL?
- a ML model
- modern reincarnation of ANN 
- the core isn't different from ANN
- collection of simple, trainable math functions
- a way we know about the brain 
- what deep for? neuron
- Each neuron is a relatively simple math function
- Working together, powerful

- a single neuron
- add nonlinearity (ReLU), b1 = max(b1,0)
- learn parameters, set expected target, min diff(target, b2) for example
- diff: a loss function that defines the diff measure (optimization)
- Gradient descent is an useful way to solve problem (non-linear function), chain rule, etc.
- back-propagation
- a way to compose and optimize a set of nonlinear funstion

# DL application by Google
- category discovery in video
- speech recognition (DL, smart way to nail it), e.g. FB, apple, etc.
- annotating images with text
- self-driving cars? (for strangers detection)
- reading text from images, street number detection

# growing use of DL at Google
- awesome

# Why DL?
- universal ML that works better than the alternatives

# Why it suddenly works so well?
- ML is a rocket engine by Andrew Ng
- Data is the rocket fuel! Data is the one
- A simple ML algorithm that scales to more data generally beats a fancier algorithm that can't

# Large datasets + powerful models
- lots of computation
- large-scale parallelism using distributed systems
- exp turnaround time and research productivity

# Model parallelism
- best way to decrease training time
- decrease the step time
- many models have lots of inherent parallelism 

- exploiting model parallelism,  single core SIMD, pretty much free
- across cores, thread parallelism, almost free, (QPI on intel)
- across devices (advanced level), for GPUs, often limited by PCIe bandwidth
- across machines, limited by network bandwidth/latency

# layer reuse
# data paralleism 
- use multiple model replicas to process different examples at the same time
- data parrllelism asynchronous distributed SGD
- speedups depend highly on kind of model
- e.g. Dense models -> 10-40X speedup from 50 replicas
- e.g. Sparse models -> support many more replicas, ~1000
- case by case, anyway

# data parallelism choices
- Synchronously, N replicas equivalent to an N times larger batch size
* pro: no gradient staleness
* con: less fault tolerant (requires some recovery if any single machine fails)
- Asynchronously
* pro: relatively fault tolerant

# considerations
- reduce parameters network
- models with fewer parameters, that resue each parameter multiple times in the computation
- mini-batches of zie B reuse parameters B times
- convolutional models
- recurrent models (LSTMs, RNNs)

# LSTMs end-to-end
- speech -> acoustics -> phonetics -> language -> text

# RankBrain
- ML the third most important individual singnal in google search ranking
- video Thumbnails

# deepdream
- transform picture from what's learned by a deep model
- learning from unlabeled images
- train on 10 million images (youtube), 1000 machines for one week 1.15 billion parameters

# tensorflow (2nd system)
- expressing high-level ML computations
- Core in C++

[C++] [Python]
[Core tensorflow execution system]
[CPU]   [GPU]   [Android]    [iOS]

# open models
# tensorflow tutorial
- tensor: n-dimensional arrays
- import tensorflow as tf

- how to define tensors
- Variable(<initual-value>,name=<>optional-name)

tf.Variable(tf.random_normal([3,3]), name='w')
y=tf.matmul(x,w)
relu_out=tf.nn.relu(y)
result = sess.run(relu_out)

                [ReLU]
                  |
[Variable] ->  [MatMul]
                  |
                  x

# code so far defines a data flow graph
# each variable corresponds to a node in the graph, not the result!!!
# needs to specify how we want to execute the graph

# Session: manage resource for graph execution
- sess.run()
# Fetch: retreve content from a node

           fetch---
                   |
---------------------------
                [ReLU]    |
                   |      |
 [Variable] ->  [MatMul]  |
                   |      |
                   x      |
---------------------------
                   |
           feed ----
# initialize variable: variable is an empty node, fill in the content of a var node

sess.run(tf.initialize_all_variables())
print sess.run(relu_out)

# How about x?
- Placeholder, its content will be fed

x = tf.placeholder("float",[1,3])

# Feed?
- pump liquid into the pipe (numpy array)

print sess.run(relu_out, feed_dict={x.np.array([[1.0,2.0,3.0]])})

# session management
- sess.close()

# Softmax?
- make prediction for n targets that sum to 1

softmax = tf.nn.softmax(relu_out)
-> unbounding to probability

# Prediction diff
# Define loss functions
- loss fun for softmax
- softmax_cross_entropy_with_logits(logits, labels, name=<optional-name>)

# Gradient descent 
- class GradientDescentOptimizer
- (learning rate)

optimizer = tf.train.GDP
train_op = optimizer.minimize(cross_entropy)

# iteratie update
- add for loop

for step in range(10):
    sess.run(train_op, feed_dict={x:np.array([[1,2,3]]),labels:answer})

# Softmax layer
# add biases

# make it deep
- add for loop, easy

# TensorBoard, visulize the graph
# save and load models
- tf.train.Saver()

# skip-gram text model
- know a word by a company

# word2Vec


