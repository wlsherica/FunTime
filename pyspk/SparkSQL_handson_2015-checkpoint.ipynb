{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SparkSQL Lab: **\n",
    "#### From this lab, you would write code to execute SQL query in Spark. Makes your analytic life simpler and faster.\n",
    "#### ** During this lab we will cover: **\n",
    "#### *Part 1:* Create a SchemaRDD (or DataFrame) \n",
    "#### *Part 2:* Loading data programmatically\n",
    "#### *Part 3:* Caching for performance\n",
    "#### *Part 4:* How many authors tagged as spam?\n",
    "#### Reference for Spark RDD [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Create a SchemaRDD (or DataFrame) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-194ef0cad76d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "spark_home = \"/opt/spark-1.4.1-bin-hadoop2.6/bin/pyspark\"\n",
    "\n",
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) DataFrame from existing RDD **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:396\n",
    "df = sc.parallelize([Row(name=\"Gordon\",beverage=\"coffee\"),\n",
    "                     Row(name=\"Katrina\",beverage=\"tea\"),\n",
    "                     Row(name=\"Graham\",beverage=\"juice\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Loading data programmatically **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Read local JSON file to DataFrame **\n",
    "#### Thank for the hashed spam data from PIXNET [PIXNET HACKATHON 2015](https://pixnethackathon2015.events.pixnet.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfile = \"file:///opt/spark-1.4.1-bin-hadoop2.6/examples/src/main/resources/people.json\"\n",
    "\n",
    "# spark 1.3\n",
    "df = sqlContext.read.json(jsonfile)\n",
    "# spark 1.4\n",
    "# DataFrame[age: bigint, name: string]\n",
    "df = sqlContext.read.load(jsonfile, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) Read data from HDFS **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "peopleDf = sqlContext.createDataFrame(people)\n",
    "peopleDf.registerTempTable(\"people\")\n",
    "# DataFrame[age: bigint, name: string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "userlog_json = \"/user/erica/userlog.txt\"\n",
    "\n",
    "userlog_info = sc.textFile(userlog_json)\n",
    "userlog_pars = userlog_info.map(lambda l: l.split(\",\"))\n",
    "userlog = userlog_pars.map(lambda p: Row(author=p[0], timestamp=int(p[1]), time=p[2], action=p[3]))\n",
    "userlogDF = sqlContext.createDataFrame(userlog)\n",
    "userlogDF.registerTempTable(\"userlog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) Read Hive table**\n",
    "####Even you didn't have hive metastore, it still works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "#error w/o hive\n",
    "sqlContext.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n",
    "sqlContext.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\")\n",
    "\n",
    "# Queries can be expressed in HiveQL.\n",
    "# DataFrame[name: string, age: bigint] -> list\n",
    "results = sqlContext.sql(\"FROM src SELECT key, value\").collect()\n",
    "\n",
    "#select *\n",
    "peopleDf.select('*').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2x) User defined functions (UDF) in Spark SQL **\n",
    "#### Don't forget the configuration of Hive should be done by placing your hive-site.xml file in conf/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an UDF for how long some text is\n",
    "#hiveCtx.registerFunction(\"strLenPython\", lambda x: len(x), IntegerType()) \n",
    "\n",
    "hiveCtx.registerFunction(\"strLenPython\", lambda x: len(x)) \n",
    "lengthSchemaRDD = hiveCtx.sql(\"SELECT strLenPython(name) FROM people\")\n",
    "\n",
    "df.registerTempTable(\"people\")\n",
    "results = sqlContext.sql(\"select name, strLenPython(name) as lengthName from people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named test_helper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ddb44aa5e435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtest_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# TEST Pluralize and test (1b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEquals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y is incorrect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named test_helper"
     ]
    }
   ],
   "source": [
    "from test_helper import Test\n",
    "# TEST Pluralize and test (1b)\n",
    "Test.assertEquals(y, 1, \"y is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2x) Saving to persistent tables**\n",
    "####  `saveAsTable ` : Saves the contents of this DataFrame to a data source as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#error: df.saveAsTable('test_table', format='csv', mode='overwrite', path='file:///') \n",
    "\n",
    "#failed on csv&txt, json works well\n",
    "peopleDf.save(path='/Users/etu/data/test_people.json', source='json', mode='overwrite')\n",
    "\n",
    "#save as local parquet file 1.4\n",
    "happySchemaRDD.write.save(\"file:///home/erica/happy.parquet\", format=\"parquet\")\n",
    "\n",
    "#save as SQL on hive\n",
    "peopleDf.saveAsTable(tableName=\"young\", source=\"parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.4 saved ok, load error later\n",
    "people.write.save(\"file:///Users/etu/data/happy.parquet\", format=\"parquet\")\n",
    "people.write.save(\"file:///Users/etu/data/happy.json\", format=\"json\")\n",
    "\n",
    "#1.4 saved and read ok\n",
    "people.write.parquet(\"file:///Users/etu/data/people2.parquet\")\n",
    "df1 = sqlContext.read.parquet(\"file:///Users/etu/data/people2.parquet\")\n",
    "\n",
    "#1.4 read example ok\n",
    "json_ex='/Users/etu/spark-1.4.1-bin-hadoop2.6/examples/src/main/resources/people.json'\n",
    "df = sqlContext.read.json(json_ex)\n",
    "\n",
    "#1.4 saved and read ok\n",
    "people.write.json(\"file:///Users/etu/data/people2.json\")\n",
    "df = sqlContext.read.json(\"file:///Users/etu/data/people2.json\")\n",
    "df2 = sqlContext.read.load(\"file:///Users/etu/data/people2.json\", format=\"json\")\n",
    "df3 = sqlConte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle_Device.ipynb\n",
      "Lucky_Draw.ipynb\n",
      "ML_lab1_review_student.ipynb\n",
      "PIXNET_Spam_2015.ipynb\n",
      "SparkSQL_handson_2015.ipynb\n",
      "UTM_user_detection.ipynb\n",
      "Vagrantfile\n",
      "lab1_word_count_student.ipynb\n",
      "mooc-setup-master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print subprocess.check_output([\"ls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: Caching for performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sqlContext.cacheTable(\"tableName\")\n",
    "sqlContext.cacheTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing RDD\n",
    "sqlContext.uncacheTable(\"tableName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: How many authors tagged as spam? **\n",
    "\n",
    "#### Use the `wordCount()` function and `takeOrdered()` to obtain top 3 most frequently author ID and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"operate_at\": 1427817600,\n",
      "        \"operate_date\": \"2015-04-01T00:00:00+08:00\",\n",
      "        \"author\": \"b1e26f5cbf4d68eee850946a7d788666bffa7cbd\",\n",
      "        \"action\": \"87a4425c5b350b685a97b5c7aa123e74599dd481\"\n",
      "    },\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_data = '/Users/etu/Desktop/kaggle/spam/user-action-log.json'\n",
    "\n",
    "print subprocess.check_output(\"cat %s | head -n 7\"%(spam_data), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "from test_helper import Test\n",
    "# TEST Pluralize and test (1b)\n",
    "y=1\n",
    "Test.assertEquals(y, 1, \"y is incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'/var/folders/v4/nb8rblts6x39m04_nsgw26vh0000gp/T/spark-98195669-7354-4221-826b-5482d70cc720/userFiles-471bdc35-9ff6-4482-ab91-7028972c6ec5', '', '/Library/Python/2.7/site-packages/pexpect-3.3-py2.7.egg', '/Library/Python/2.7/site-packages/gnureadline-6.3.3-py2.7-macosx-10.9-intel.egg', '/Library/Python/2.7/site-packages/appnope-0.1.0-py2.7.egg', '/Library/Python/2.7/site-packages/traitlets-4.0.0.dev-py2.7.egg', '/Library/Python/2.7/site-packages/simplegeneric-0.8.1-py2.7.egg', '/Library/Python/2.7/site-packages/pickleshare-0.5-py2.7.egg', '/Library/Python/2.7/site-packages/decorator-3.4.2-py2.7.egg', '/Library/Python/2.7/site-packages/ipython_genutils-4.0.0.dev1-py2.7.egg', '/Library/Python/2.7/site-packages/path.py-7.3-py2.7.egg', '/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg', '/Library/Python/2.7/site-packages/numpy-1.9.2-py2.7-macosx-10.9-intel.egg', '/Users/etu/spark-1.3.0/python/lib/py4j-0.8.2.1-src.zip', '/Users/etu/spark-1.3.0/python', '/Users/etu/myvagrant', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages', '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload', '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/PyObjC', '/Library/Python/2.7/site-packages', '/Library/Python/2.7/site-packages/IPython/extensions', '/Users/etu/.ipython', '/Users/etu/FunTime/spkgroup/test_helper', '/Users/etu/FunTime/spkgroup/test_helper']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/etu/FunTime/spkgroup/test_helper\")\n",
    "print sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
