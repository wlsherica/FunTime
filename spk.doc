cd spark-1.0.2-bin-hadoop2
./bin/spark-shell

--linux command on gz file
zcat Brand.dump.gz | wc -l
zcat Brand.dump.gz | head -n 1

--list file name
find /data/reallyLight/dumpData -type f -printf "%f\n"

--start workder
./bin/spark-class org.apache.spark.deploy.worker.Worker spark://cdh4-dn1:7077

sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/text_mining/forum/PTTCar/20140621.xml.hadoop")
val file = sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/text_mining/forum/PTTCar/20140621.xml.hadoop")
file.count()

---
./bin/pyspark
textFile = sc.textFile("hdfs://cdh4-n.migosoft.com/user/athena/data_prepare")
textFile.count()


textFile = sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/text_mining/forum/PTTCar/20140621.xml.hadoop")

./bin/spark-class org.apache.spark.deploy.worker.Worker spark://cdh4-dn1:7077


wget http://d3kbcqa49mib13.cloudfront.net/spark-1.0.2-bin-hadoop2.tgz
---
#.14
cd /data/spark/spark-1.0.2-bin-hadoop2
./bin/pyspark

test=sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/reallyLight/Guest.dump")

#--testing for DB################################
test=sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/el16_test/guest.txt")
test.count()

#filter
va=nums.filter(lambda x: re.match("^\d+.+?$"Â¢x)[5],1)).reduceByKey(add),x)!=None)

va=nums.filter(lambda x: re.match("^\d+.+?$",x)!=None).map(lambda word:(word,1)).reduceByKey(lambda x,y:x+y)

va=nums.filter(lambda x: re.match("^\d+.+?$",x)!=None).reduce(lambda x,y:float(x)+float(y))

data=test.map(lambda x:(x.replace("'",'').split('\t')[14].encode('utf-8'))).reduce(lambda x,y: float(x)+float(y))

#max&min
>>> def maxx(a, b):
...     if a > b:
...         return a
...     else:
...         return b
...

>>> def minn(a, b):
...     if a < b:
...         return a
...     else:
...         return b
...

sss=test.map(lambda x:(float(x.replace("'",'').split('\t')[14].encode('utf-8')))).reduce(minn)
->print result
sss=test.map(lambda x:(float(x.replace("'",'').split('\t')[14].encode('utf-8')))).reduce(maxx)

## SD
def var(a,avg1):
    return (a-avg1)**2

tmp=test.map(lambda x:(var((x.replace("'",'').split('\t')[14].encode('utf-8')),avg1))).reduce(lambda x: x/

2627226.4473668276, 2251416.5995357353, 2627226.4473668276


sss=test.map(lambda x:(x.replace("'",'').split('\t')[1].encode('utf-8')))


#2D


data=test.map(lambda x:(x.replace("'",'').split('\t')[1].encode('utf-8'),float(x.replace("'",'').split('\t')[14].encode('utf-

8')))).reduceByKey(add)


#product
list(itertools.combinations(col_list.keys(),2))
#[(1, 2), (1, 3), (2, 3)]
col2=list(itertools.product(col, repeat=2))

col2=list(itertools.permutations(col,2))
#[(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]

if not a or not b:

dict((x,y) for x,y in card) 
->{u'80,': 10940, u'72,': 299}


class AutoVivification(dict):
    """Implementation of perl's autovivification feature."""
    def __getitem__(self, item):
        try:
            return dict.__getitem__(self, item)
        except KeyError:
            value = self[item] = type(self)()
            return value

a = AutoVivification()

a[1][2][3] = 4
a[1][3][3] = 5
a[1][2]['test'] = 6

print a
#{1: {2: {'test': 6, 3: 4}, 3: {3: 5}}}

# 5.16s
cardType=test.map(lambda x:(x.split(',')[1],1)).reduceByKey(add)
cardType.collect()

#sum(paymoney) ans=1248590662.91, 5s
data=test.map(lambda x:(x.split(',')[14].encode('utf-8')))

data=test.map(lambda x:(x.replace("'",'').split('\t')[14].encode('utf-8')))
data2=data.filter(lambda x: x.isalpha()==False)
data3=data2.collect()
nums=sc.parallelize(data3)
nums.reduce(lambda x, y: float(x)+float(y))

#or you can do:, print data->1248590662.9099917
data=test.map(lambda x:(x.replace("'",'').split('\t')[14].encode('utf-8'))).reduce(lambda x, y: float(x)+float(y))
#w/o collect

filter(lambda x:x%2, data)
filter(lambda x: x.isalpha()==False,s)

##############################################################

nums = sc.parallelize(['11','2.0', 'a3','3.42','99'])

nums.takeOrdered(3)
-> 11,2,3.42

va=nums.filter(lambda x: re.match("^\d+.+?$",x)!=None).reduce(lambda x,y:float(x)+float(y)) 
-> va=115.42


va=nums.filter(lambda x: re.match("^\d+.+?$",x)!=None).map(lambda x:(float(x),1)).sortByKey()
va.take(1)

va.take(1)[0][0] 
-> 2

va=nums.filter(lambda x: re.match("^\d+.+?$",x)!=None).map(lambda x:(float(x),1)).sortByKey(True)
-> default sorting 

va=nums.filter(lambda x: re.match("^\d+.+?$",x)!=None).map(lambda x:(float(x),1)).sortByKey(False)
-> descending


##################################################################################
weather=sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/el16_test/training.1.txt")
weather.count() 

lines = sc.textFile("data.txt")
pairs = lines.map(lambda s: (s, 1))
counts = pairs.reduceByKey(lambda a, b: a + b)
counts.collect()


lines=sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/el16_test/tempr.txt")
cusco=lines.filter(lambda line: "CUSCO" in line)
cusco.count() #12

>>> file=sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/el16_test/ebola.txt")
>>> counts=file.flatMap(lambda line: line.split(" ")).map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b)
>>> counts

--or you can do:
from operator import add
q=weather.flatMap(lambda x: x.split(",")).map(lambda x: (x,1)).reduceByKey(add)
outp=q.collect()
for i in outp:
    print i 

---take first line
q.weather.take(1)

---take first column
q=weather.map(lambda x:x.split(",")[0])
---group by func
q=weather.map(lambda x:(x.split(",")[0],1)).reduceByKey(add)

#textFile("/my/directory"), textFile("/my/directory/*.txt"), and textFile("/my/directory/*.gz").

---
cd /data/spark/spark-1.0.2-bin-hadoop2
./bin/pyspark /home/training/erica/simpleapp.py
./bin/pyspark /home/training/erica/wordcnt_spk.py
#ines with a: 73, lines with b: 35

./bin/spark-submit /home/training/erica/pyspk/test_2d.py
./bin/pyspark /home/training/erica/pyspk/test_2d.py


--log mining
val a=sc.textFile("hdfs://cdh4-n.migosoft.com/user/training/el16_test/training.1.txt")
var err = a.filter(t=>t.contains("2014")
err.cache()
val m=err.filter(t=>t.contains("MYSQL")).count()

----json format 
import json

>>> class student:
...     def __init__(self,name,email,ug=None,pg=None):
...         self.name=name
...         self.email=email
...         self.edu={"ug":[ug],"pg":[pg]}

james=student("james","s@s.com","cs","cs")
print json.dumps(vars(james),sort_keys=True,indent=4)
{
    "edu": {
        "pg": [
            "cs"
        ], 
        "ug": [
            "cs"
        ]
    }, 
    "email": "s@s.com", 
    "name": "james"
}

#check alpha or not
try:
    float(element)
except ValueError:
    print "Not a float"
#or
import re
if re.match("^\d+?\.\d+?$", element) is None:
    print "Not float"

#
import itertools
list(itertools.combination(b,2))

    for i in col_list:
        tmp=logData.map(lambda x:(x.split('\t')[i],1)).reduceByKey(add)
        for(word, count) in tmp.collect():
            print "%s: %i" % (word, count)





raw = logFile.map(lambda x:(parserLine(x)[1].encode('utf-8')))



/usr/bin/spark-submit 2d.py -d "14:money:C,1:cardType:N"











