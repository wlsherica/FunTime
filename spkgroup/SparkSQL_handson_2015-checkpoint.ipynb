{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SparkSQL Lab: **\n",
    "#### From this lab, you would write code to execute SQL query in Spark. Makes your analytic life simpler and faster.\n",
    "#### ** During this lab we will cover: **\n",
    "#### *Part 1:* Create a SchemaRDD (or DataFrame) \n",
    "#### *Part 2:* Loading data programmatically\n",
    "#### *Part 3:* Caching for performance\n",
    "#### *Part 4:* How many authors tagged as spam?\n",
    "#### Reference for Spark RDD [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Create a SchemaRDD (or DataFrame) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-194ef0cad76d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) DataFrame from existing RDD **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sc.parallelize([Row(name=\"Gordon\",beverage=\"coffee\"),\n",
    "                     Row(name=\"Katrina\",beverage=\"tea\"),\n",
    "                     Row(name=\"Graham\",beverage=\"juice\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Loading data programmatically **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Read local JSON file to DataFrame **\n",
    "#### Thank for the hashed spam data from PIXNET [PIXNET HACKATHON 2015](https://pixnethackathon2015.events.pixnet.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark 1.3\n",
    "df = sqlContext.read.json(\"examples/src/main/resources/people.json\")\n",
    "# spark 1.4\n",
    "#df = sqlContext.read.load(\"examples/src/main/resources/people.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) Read data from HDFS **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "schemaPeople.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) Read Hive table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "sqlContext.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n",
    "sqlContext.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\")\n",
    "\n",
    "# Queries can be expressed in HiveQL.\n",
    "results = sqlContext.sql(\"FROM src SELECT key, value\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2x) User defined functions (UDF) in Spark SQL **\n",
    "#### Don't forget the configuration of Hive should be done by placing your hive-site.xml file in conf/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an UDF for how long some text is\n",
    "hiveCtx.registerFunction(\"strLenPython\", lambda x: len(x), IntegerType()) \n",
    "lengthSchemaRDD = hiveCtx.sql(\"SELECT strLenPython('text') FROM tweets LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named test_helper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ddb44aa5e435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtest_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# TEST Pluralize and test (1b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEquals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y is incorrect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named test_helper"
     ]
    }
   ],
   "source": [
    "from test_helper import Test\n",
    "# TEST Pluralize and test (1b)\n",
    "Test.assertEquals(y, 1, \"y is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2x) Saving to persistent tables**\n",
    "####  `saveAsTable ` : Saves the contents of this DataFrame to a data source as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.saveAsTable('test_table', format='csv', mode='overwrite', path='file:///') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle_Device.ipynb\n",
      "Lucky_Draw.ipynb\n",
      "ML_lab1_review_student.ipynb\n",
      "PIXNET_Spam_2015.ipynb\n",
      "SparkSQL_handson_2015.ipynb\n",
      "UTM_user_detection.ipynb\n",
      "Vagrantfile\n",
      "lab1_word_count_student.ipynb\n",
      "mooc-setup-master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print subprocess.check_output([\"ls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: Caching for performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.cacheTable(\"tableName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.uncacheTable(\"tableName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: How many authors tagged as spam? **\n",
    "\n",
    "#### Use the `wordCount()` function and `takeOrdered()` to obtain top 3 most frequently author ID and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"operate_at\": 1427817600,\n",
      "        \"operate_date\": \"2015-04-01T00:00:00+08:00\",\n",
      "        \"author\": \"b1e26f5cbf4d68eee850946a7d788666bffa7cbd\",\n",
      "        \"action\": \"87a4425c5b350b685a97b5c7aa123e74599dd481\"\n",
      "    },\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_data = '/Users/etu/Desktop/kaggle/spam/user-action-log.json'\n",
    "\n",
    "print subprocess.check_output(\"cat %s | head -n 7\"%(spam_data), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from test_helper import Test\n",
    "# TEST Pluralize and test (1b)\n",
    "Test.assertEquals(y, 1, \"y is incorrect\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
